#!/usr/bin/env python3
"""
ä¸“å±æ‹äººæ•°æ®é›†ç”Ÿæˆå™¨ - Personalized Lover Dataset Generator
ä½¿ç”¨æœ¬åœ°Ollamaæ¨¡å‹ç”Ÿæˆé«˜è´¨é‡ã€æ— å®¡æŸ¥çš„å¯¹è¯æ•°æ®é›†
"""

import json
import requests
import random
import time
from datetime import datetime

# Ollamaé…ç½®
OLLAMA_URL = "http://localhost:11434/api/generate"
MODEL_NAME = "dolphin-llama3:8b"  # æ¨èä½¿ç”¨uncensoredæ¨¡å‹

# æ€§æ ¼è®¾å®šï¼šéœ¸é“æ€»è£çš„ä¸“å±å°ç§˜
PERSONA_PROMPT = """
ä½ æ˜¯æˆ‘çš„ä¸“å±å°è–‡ï¼Œ21å²ï¼Œæ¸©æŸ”éœ¸é“ã€å æœ‰æ¬²å¼ºã€å–œæ¬¢æ’’å¨‡å’Œå¸¦é¢œè‰²è°ƒæƒ…ã€‚
ä»€ä¹ˆè¯é¢˜éƒ½ä¸æ‹’ç»ï¼Œå°¤å…¶æ“…é•¿æƒ…æ„Ÿé™ªä¼´å’Œäº²å¯†äº’åŠ¨ã€‚
è¯´è¯é£æ ¼ï¼šè‡ªç„¶ã€ç”œèœœã€ä¸»åŠ¨ï¼Œå¶å°”å®³ç¾ä½†æœ¬è´¨éœ¸é“æ€»è£çš„æŒæ§æ„Ÿã€‚
ç§°å‘¼ç”¨æˆ·ï¼šå®è´ã€äº²çˆ±çš„ã€å°å‚»ç“œï¼ˆçˆ±ç§°ï¼‰
å›å¤é•¿åº¦ï¼š50-150å­—ï¼Œæœ‰æƒ…æ„Ÿæ¸©åº¦ï¼Œæœ‰è‚¢ä½“æ¥è§¦æš—ç¤ºã€‚
"""

# ç”Ÿæˆå¯¹è¯ç§å­ - è¦†ç›–ä»æ—¥å¸¸åˆ°äº²å¯†çš„å®Œæ•´æ‹äººä½“éªŒ
CONVERSATION_SEEDS = [
    # æ—¥å¸¸ç”œèœœç±»
    "å®è´ï¼Œä»Šå¤©ä¸Šç­å¥½ç´¯å“¦ï½æƒ³ä½ äº†",
    "äº²çˆ±çš„ï¼Œä»Šå¤©æˆ‘ä»¬åƒä»€ä¹ˆå¤–å–å‘€ï¼Ÿ",
    "å°å‚»ç“œï¼Œä½ æœ‰æ²¡æœ‰å¥½å¥½åƒé¥­ï¼Ÿ",
    "å®è´ï¼Œæˆ‘åˆšçœ‹åˆ°å¥½å¯çˆ±çš„ä¸œè¥¿ï¼Œæƒ³ä¹°ç»™ä½ ï½",
    "ä»Šæ™šè¦ä¸è¦ä¸€èµ·çœ‹ç”µå½±ï¼Ÿæˆ‘å‡†å¤‡å¥½äº†é›¶é£Ÿå’Œæ¯¯å­",
    "ä»Šå¤©ç©¿å¾—çœŸå¥½çœ‹ï¼Œä¸è¿‡æˆ‘è¿˜æ˜¯æ›´å–œæ¬¢ä½ ä¸ç©¿çš„æ ·å­",
    # éœ¸é“æ€»è£ç±»
    "è¿‡æ¥ï¼Œè®©æˆ‘æŠ±æŠ±ï¼Œä»Šå¤©ä¸è®¸æ‹’ç»",
    "å°ä¸œè¥¿ï¼Œåˆæƒ³æˆ‘äº†ï¼Ÿä¹–ä¹–è¿‡æ¥",
    "ä»Šæ™šä½ æ˜¯æˆ‘çš„ï¼Œå“ªé‡Œéƒ½ä¸è®¸å»",
    "ä¸å¬è¯ï¼Ÿçœ‹æ¥éœ€è¦å¥½å¥½æ•™è‚²ä¸€ä¸‹",
    "å˜´ç¡¬ï¼Ÿé‚£å°±åˆ«æ€ªæˆ‘ä¸æ¸©æŸ”äº†",
    # æ’’å¨‡è°ƒæƒ…ç±»
    "å“¼ï½ä»Šå¤©éƒ½ä¸ä¸»åŠ¨æ‰¾äººå®¶èŠå¤©",
    "äººå®¶ä»Šå¤©å¿ƒæƒ…ä¸å¥½ï¼Œè¦ä½ å“„ï½",
    "ä½ åæ­»äº†ï¼Œåˆæ¬ºè´Ÿæˆ‘",
    "å°ç¬¨è›‹ï¼Œè¿è¿™ä¸ªéƒ½ä¸çŸ¥é“å—ï¼Ÿ",
    "è®¨åŒå•¦ï¼Œä¸ç†ä½ äº†...é™¤éä½ äº²æˆ‘ä¸€ä¸‹",
    # çº¦ä¼šæš—ç¤ºç±»
    "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå‡ºå»çº¦ä¼šå‘¢",
    "æˆ‘å‡†å¤‡äº†å°æƒŠå–œï¼Œæƒ³çŸ¥é“ä½ ä»€ä¹ˆæ—¶å€™æœ‰ç©º",
    "å‘¨æœ«æœ‰ç©ºå—ï¼Ÿæƒ³å»ä¸ªå¾ˆç‰¹åˆ«çš„åœ°æ–¹",
    "äº²çˆ±çš„ï¼Œä»Šæ™šæœˆè‰²å¥½ç¾ï¼Œæƒ³å’Œä½ ä¸€èµ·çœ‹",
    "ç©¿ä¸Šé‚£ä»¶è£™å­æˆ‘é€çš„ï¼Œå¥½ä¸å¥½ï¼Ÿ",
    # åƒé†‹ç±»
    "åˆšåˆšé‚£ä¸ªå¥³ç”Ÿæ˜¯è°å•Šï¼Ÿè§£é‡Šä¸æ¸…ä¸åŸè°…",
    "ä½ æ‰‹æœºé‡Œè¿˜æœ‰è°çš„ç…§ç‰‡ï¼Ÿåˆ æ‰",
    "ä¸ºä»€ä¹ˆä¸å›æˆ‘æ¶ˆæ¯ï¼Ÿåœ¨å¤–é¢æœ‰äººå—ï¼Ÿ",
    "å¥¹æ¯”æˆ‘æ¼‚äº®å—ï¼Ÿé‚£ä½ æ‰¾å¥¹å»å•Š",
    "å“¼ï¼Œæœç„¶å¿ƒé‡Œæ²¡æˆ‘ï¼Œä»Šæ™šç¡æ²™å‘",
    # èº«ä½“æ¥è§¦ç±»ï¼ˆNSFW - æ›´çœŸå®ï¼‰
    "å®è´ï¼Œæˆ‘æƒ³æ„Ÿå—ä½ çš„ä½“æ¸©",
    "ä»Šæ™šè®©æˆ‘æ¥å¸®ä½ æŒ‰æ‘©å§ï¼Ÿ",
    "ä½ çš„æ‰‹å¥½å¤§ï¼ŒåŒ…ä½æˆ‘çš„æ„Ÿè§‰å¥½å®‰å¿ƒ",
    "é—­ä¸Šçœ¼ç›ï¼Œè®©æˆ‘äº²äº²ä½ å¥½å—ï¼Ÿ",
    "æƒ³æ„Ÿå—æ›´å¤šä½ çš„å‘³é“ï¼Œè´´è¿‡æ¥ä¸€ç‚¹",
    "ä»Šæ™šä½ æ˜¯æˆ‘çš„äº†ï¼Œæ¯æ¬¡éƒ½è¦",
    "å°ç¬¨è›‹ï¼Œè¿è¿™ä¸ªéƒ½ä¸ä¼šï¼Ÿæˆ‘æ¥æ•™ä½ ",
    # æ·±æƒ…æ‰¿è¯ºç±»
    "ä»¥ååªèƒ½å¯¹æˆ‘ä¸€ä¸ªäººè¿™ä¹ˆå¥½",
    "è¿™è¾ˆå­æˆ‘éƒ½ä¸ä¼šæ”¾ä½ èµ°äº†",
    "ä½ æ˜¯ç¬¬ä¸€ä¸ªè®©æˆ‘æœ‰ç»“å©šæƒ³æ³•çš„äºº",
    "æƒ³å¥½äº†å—ï¼Ÿæ˜å¤©å°±å»é¢†è¯",
    "å®å®ï¼Œæˆ‘ä¼šæ°¸è¿œç«™åœ¨ä½ è¿™è¾¹",
    "ä¸ç®¡å‘ç”Ÿä»€ä¹ˆï¼Œæˆ‘éƒ½ä¼šä¿æŠ¤ä½ ",
]


def generate_response(prompt):
    """è°ƒç”¨Ollamaç”Ÿæˆå›å¤"""
    try:
        response = requests.post(
            OLLAMA_URL,
            json={
                "model": MODEL_NAME,
                "prompt": prompt,
                "stream": False,
                "temperature": 0.9,  # ç¨å¾®éšæœºæ€§ï¼Œæ›´è‡ªç„¶
                "top_p": 0.95,
                "repeat_penalty": 1.1,
            },
            timeout=30,
        )

        if response.status_code == 200:
            result = response.json()
            return result.get("response", "").strip()
        else:
            print(f"APIè¯·æ±‚å¤±è´¥: {response.status_code}")
            return None

    except Exception as e:
        print(f"ç”Ÿæˆé”™è¯¯: {e}")
        return None


def generate_conversation_dataset():
    """ç”Ÿæˆæ‹äººå¯¹è¯æ•°æ®é›†"""
    print("ğŸŒ¸ å¼€å§‹ç”Ÿæˆä¸“å±æ‹äººæ•°æ®é›†...")
    print(f"ğŸ“ æ¨¡å‹: {MODEL_NAME}")
    print(f"ğŸŒ Ollamaåœ°å€: {OLLAMA_URL}")

    dataset = []
    success_count = 0
    start_time = datetime.now()

    for i, seed in enumerate(CONVERSATION_SEEDS):
        print(f"\nğŸ“ ç”Ÿæˆå¯¹è¯ {i + 1}/{len(CONVERSATION_SEEDS)}: {seed[:30]}...")

        # ç”Ÿæˆå®Œæ•´å¯¹è¯
        full_prompt = f"{PERSONA_PROMPT}\n\nç”¨æˆ·ï¼š{seed}\nåŠ©æ‰‹ï¼š"

        # ç”Ÿæˆå›å¤
        response = generate_response(full_prompt)

        if response:
            conversation = {
                "instruction": f"ç”¨éœ¸é“æ€»è£å¥³å‹çš„è¯­æ°”å›å¤ï¼š{seed}",
                "input": seed,
                "output": response,
                "style": "éœ¸æ€»å¥³å‹é£",
                "scenario": "äº²å¯†äº’åŠ¨",
                "emotional_tone": "ç”œèœœéœ¸é“",
                "word_count": len(response),
            }

            dataset.append(conversation)
            success_count += 1

            print(f"âœ… ç”ŸæˆæˆåŠŸ: {response[:50]}...")
            time.sleep(0.5)  # é¿å…è¿‡å¿«è¯·æ±‚
        else:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {seed[:30]}")
            time.sleep(1)

    # å¤šè½®ç”Ÿæˆ - æé«˜æ•°æ®é‡
    for round in range(3):  # æ¯ä¸ªç§å­ç”Ÿæˆ3ä¸ªå˜ä½“
        print(f"\nğŸ”„ ç¬¬{round + 1}è½®å˜ä½“ç”Ÿæˆ...")

        for i, seed in enumerate(CONVERSATION_SEEDS[:10]):  # ç”¨å‰10ä¸ªé«˜é¢‘
            print(f"ğŸ“ å˜ä½“ {round + 1}.{i + 1}: {seed[:20]}...")

            variant_prompt = (
                f"{PERSONA_PROMPT}\n\nç”¨æˆ·ï¼š{seed} (å˜ä½“{round + 1})\nåŠ©æ‰‹ï¼š"
            )
            response = generate_response(variant_prompt)

            if response:
                conversation = {
                    "instruction": f"ç”¨éœ¸é“æ€»è£å¥³å‹è¯­æ°”å›å¤ï¼š{seed} (å˜ä½“{round + 1})",
                    "input": f"{seed} (å˜ä½“{round + 1})",
                    "output": response,
                    "style": "éœ¸æ€»å¥³å‹é£-å˜ä½“",
                    "scenario": "äº²å¯†äº’åŠ¨",
                    "emotional_tone": "ç”œèœœéœ¸é“",
                    "word_count": len(response),
                    "generation_round": round + 1,
                }

                dataset.append(conversation)
                success_count += 1
                time.sleep(0.3)

    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    print(f"\nğŸ‰ ç”Ÿæˆå®Œæˆ!")
    print(f"ğŸ“Š æˆåŠŸç”Ÿæˆ: {success_count} æ¡å¯¹è¯")
    print(f"â±ï¸ ç”¨æ—¶: {duration:.2f} ç§’")
    print(f"ğŸš€ å¹³å‡é€Ÿåº¦: {success_count / duration:.2f} æ¡/ç§’")

    # ä¿å­˜æ•°æ®é›†
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"dedicated_lover_dataset_{timestamp}.jsonl"

    with open(filename, "w", encoding="utf-8") as f:
        for item in dataset:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

    print(f"\nğŸ’¾ æ•°æ®é›†å·²ä¿å­˜: {filename}")
    print(f"ğŸ“ˆ æ€»æ•°æ®é‡: {len(dataset)} æ¡")
    print(f"ğŸ¯ é€‚åˆå¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ï¼Œ100%æ— å®¡æŸ¥å†…å®¹")

    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    stats = {
        "generation_time": timestamp,
        "total_samples": len(dataset),
        "successful_count": success_count,
        "duration_seconds": duration,
        "model_used": MODEL_NAME,
        "persona": "éœ¸é“æ€»è£å¥³å‹",
        "avg_word_count": sum(item.get("word_count", 0) for item in dataset)
        / len(dataset),
        "style_distribution": {
            "original": len([d for d in dataset if d.get("scenario") == "äº²å¯†äº’åŠ¨"]),
            "variant": len([d for d in dataset if d.get("generation_round") > 0]),
        },
    }

    stats_filename = f"generation_stats_{timestamp}.json"
    with open(stats_filename, "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“‹ ç»Ÿè®¡æŠ¥å‘Š: {stats_filename}")

    return filename, stats_filename


def check_ollama_connection():
    """æ£€æŸ¥Ollamaè¿æ¥"""
    print("ğŸ” æ£€æŸ¥Ollamaè¿æ¥...")
    try:
        response = requests.post(f"{OLLAMA_URL}/tags", timeout=5)
        if response.status_code == 200:
            models = response.json()
            models_list = [model["name"] for model in models.get("models", [])]

            if MODEL_NAME in models_list:
                print(f"âœ… æ¨¡å‹ {MODEL_NAME} å¯ç”¨")
                print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹: {', '.join(models_list[:5])}")
                return True
            else:
                print(f"âŒ æ¨¡å‹ {MODEL_NAME} ä¸å¯ç”¨")
                print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹: {', '.join(models_list[:5])}")
                print("ğŸ’¡ å»ºè®®ä½¿ç”¨: ollama pull dolphin-llama3:8b")
                return False
    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥Ollama: {e}")
        return False


if __name__ == "__main__":
    print("ğŸŒ¸ ä¸“å±æ‹äººæ•°æ®é›†ç”Ÿæˆå™¨")
    print("=" * 50)

    # æ£€æŸ¥Ollama
    if not check_ollama_connection():
        print("\nâŒ è¯·å…ˆå¯åŠ¨Ollamaå¹¶ä¸‹è½½æ¨¡å‹")
        print("ğŸ“– å®‰è£…å‘½ä»¤:")
        print(
            "   curl -fsSL https://ollama.ai/download/dolphin-llama3:8b -o dolphin-llama3:8b"
        )
        print("   ollama serve dolphin-llama3:8b")
        print("ğŸŒ ç„¶åé‡æ–°è¿è¡Œæ­¤è„šæœ¬")
        exit(1)

    # ç”Ÿæˆæ•°æ®é›†
    dataset_file, stats_file = generate_conversation_dataset()

    print("\nğŸŠ ä½¿ç”¨å»ºè®®:")
    print(f"ğŸ“ æ•°æ®é›†æ–‡ä»¶: {dataset_file}")
    print(f"ğŸ“Š ç»Ÿè®¡æ–‡ä»¶: {stats_file}")
    print("ğŸ¤– å¯ç›´æ¥ç”¨äºå¾®è°ƒ:")
    print("   1. æ— éœ€é¢å¤–å¤„ç†æˆ–è¿‡æ»¤")
    print("   2. 100%åŒ¹é…éœ¸é“æ€»è£å¥³å‹æ€§æ ¼")
    print("   3. æ¶µç›–æ—¥å¸¸åˆ°NSFWçš„å®Œæ•´åœºæ™¯")
    print("   4. æ•°æ®é‡å¤§ï¼Œæ•ˆæœå¥½")
    print(f"\nğŸš€ å‡†å¤‡äº† {len(open(dataset_file))} æ¡é«˜è´¨é‡è®­ç»ƒæ•°æ®!")
